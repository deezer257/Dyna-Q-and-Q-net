{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this part of the assignment is to introduce you to the concept of Dyna (Sutton, 1990). By now you will have heard about model-free (MF) and model-based (MB) approaches to control. MF algorithms learn and store estimates of the state-action value function. This means that control simply involves retrieving those cached value estimates and choosing among the available actions by comparing their worth. MB control, on the other hand, involves the use of a model to calculate those values. In the RL literature this is typically known as planning. Planning has the advantage of affording behavioural flexibility, since once a change in the world is discovered the model can be updated and the values re-calculated in a way that reflects the global knowledge of the environment. By contrast, MF algorithms need many experiences to propagate the information about the change to other states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dyna (Sutton, 1990) is an integrated architecture which combines the merits of MF and MB approaches. Whilst interacting with the environment (i.e., being online), Dyna learns MF state-action values as well as a model of its environment (in the most general sense this involves both the transition and reward models). Whilst not interacting with the environment (i.e., being offline -- this can be in between consecutive moves or episodes, or during the equivalent of sleep in animals), Dyna uses its learnt model to additinally train the MF values. That is, the model now acts as a simulator of the environment and provides additional experiences for learning. This means that at decision time Dyna is fast to react, since it acts according to an MF policy by simply retrieving the relevant values; however, those values have been trained by a model and therefore contain some portion of the global knowledge of the environment collected so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particularly critical aspect of learning is exploration, and in fact Dyna's original motivation was to improve the efficiency of exploration. You are invited to read the original paper by Sutton (1990) to familiarise yourself with the idea and the sort of problems it attempts to solve. The pdf of the paper can be found in the `papers` folder of this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 [10 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the assignment is to reproduce some of the results from the original Dyna paper (Sutton, 1990). In particular, **your task is to generate and visualise data plotted in figure 6** in that paper. You can neglect Dyna-PI and only implement Dyna-$Q$- and Dyna-$Q$+.\n",
    "\n",
    "To make your life a little easier, and to let you jump right into the more interesting stuff, you are provided with the environment simulator located in `environment.py`, as well as a blueprint of the main code for the agent. That is, you have access to the file `agent.py` where you will find the `DynaAgent` class. This class has a method called `simulate` with the main simulation loop already implemented.\n",
    "\n",
    "**Your task is to fill in the missing implementation** in the `agent.py` file. Thus, you are tasked to complete the following functions:\n",
    "- `_policy`. This is the typical $\\pi(a\\mid s)$ which specifies how the agent chooses actions in any given state\n",
    "- `_update_qvals`. This is the $Q$-value update rule\n",
    "- `_update_experience_buffer`. This updates the agent's experience buffer from which it then samples planning updates\n",
    "- `_update_action_count`. This counts the number of moves elapsed since each action has last been attempted\n",
    "- `_plan`. This is the function which lets the agent plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that's done you can run the code below which will hopefully reproduce the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from write_load import load_env\n",
    "from agent import DynaAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environments\n",
    "maze_conf_path = os.path.abspath(os.path.join(os.getcwd(), 'envs'))\n",
    "maze1_conf = load_env(os.path.join(maze_conf_path, 'dyna1.txt')) # maze with only the right path open \n",
    "maze2_conf = load_env(os.path.join(maze_conf_path, 'dyna2.txt')) # maze with both paths open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the agents\n",
    "# note that alpha is the learning rate (instead of beta as in the paper)\n",
    "dyna_qplus  = DynaAgent(alpha=0.5, gamma=0.9, epsilon=0.001)\n",
    "dyna_qminus = DynaAgent(alpha=0.5, gamma=0.9, epsilon=0)\n",
    "\"\"\" Introduce an epsilon greedy agent. Best action is chosen and sometimes exploration. \"\"\"\n",
    "eps_greedy = DynaAgent(alpha=0.5, gamma=0.9, epsilon=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\flori\\Dokumente\\Studium Tuebingen\\Inhalt\\Master\\1. Semester\\Neural Modelling\\Woche 9\\assignment.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx_run \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_runs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     agent\u001b[39m.\u001b[39minit_env(maze1_conf)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     agent\u001b[39m.\u001b[39;49msimulate(num_trials\u001b[39m=\u001b[39;49mnum_trials, reset_agent\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_planning_updates\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# world change\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     agent\u001b[39m.\u001b[39minit_env(maze2_conf)\n",
      "File \u001b[1;32mc:\\Users\\flori\\Dokumente\\Studium Tuebingen\\Inhalt\\Master\\1. Semester\\Neural Modelling\\Woche 9\\agent.py:248\u001b[0m, in \u001b[0;36mDynaAgent.simulate\u001b[1;34m(self, num_trials, reset_agent, num_planning_updates)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_state\n\u001b[0;32m    245\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_trials):\n\u001b[0;32m    246\u001b[0m \n\u001b[0;32m    247\u001b[0m     \u001b[39m# choose action\u001b[39;00m\n\u001b[1;32m--> 248\u001b[0m     a  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ms)\n\u001b[0;32m    249\u001b[0m     \u001b[39m# get into new state\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     s1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_new_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ms, a)            \n",
      "File \u001b[1;32mc:\\Users\\flori\\Dokumente\\Studium Tuebingen\\Inhalt\\Master\\1. Semester\\Neural Modelling\\Woche 9\\agent.py:180\u001b[0m, in \u001b[0;36mDynaAgent._policy\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    177\u001b[0m p_a_given_s \u001b[39m=\u001b[39m []\n\u001b[0;32m    179\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Use softmax function to calulate the probability of each action.\"\"\"\u001b[39;00m           \n\u001b[1;32m--> 180\u001b[0m denominator \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha  \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(Q_explore[s,:]))\n\u001b[0;32m    182\u001b[0m \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m Q_all_action:\n\u001b[0;32m    183\u001b[0m     p_a_given_s \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha  \u001b[39m*\u001b[39m Q_explore[s,action]) \u001b[39m/\u001b[39m denominator]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "# run simulations\n",
    "num_trials = 3000\n",
    "num_runs   = 50\n",
    "perf       = [np.zeros((num_runs, num_trials*2)),np.zeros((num_runs, num_trials*2)), np.zeros((num_runs, num_trials*2))]\n",
    "agents     = [dyna_qplus, dyna_qminus, eps_greedy]\n",
    "\n",
    "for idx_agent, agent in enumerate(agents):\n",
    "\n",
    "    for idx_run in range(num_runs):\n",
    "        agent.init_env(maze1_conf)\n",
    "        agent.simulate(num_trials=num_trials, reset_agent=True, num_planning_updates=10)\n",
    "        # world change\n",
    "        agent.init_env(maze2_conf)\n",
    "        agent.simulate(num_trials=num_trials, reset_agent=False, num_planning_updates=10)\n",
    "        # save performance\n",
    "        perf[idx_agent][idx_run, :] = agent.get_performace()\n",
    "        if (idx_run+1)%10 == 0:\n",
    "            print('done with run %u/%u'%(idx_run+1, num_runs))\n",
    "\n",
    "# average cumulative reward\n",
    "avg_perf_dyna_qplus  = np.mean(perf[0], axis=0)\n",
    "avg_perf_dyna_qminus = np.mean(perf[1], axis=0)\n",
    "avg_perf_eps_greedy = np.mean(perf[2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_perf_dyna_qplus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\flori\\Dokumente\\Studium Tuebingen\\Inhalt\\Master\\1. Semester\\Neural Modelling\\Woche 9\\assignment.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# plot Figure 6 from Sutton (1990) here\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m6\u001b[39m, \u001b[39m4\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(avg_perf_dyna_qplus,  label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDyna-Q+\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(avg_perf_dyna_qminus, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDyna-Q-\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(avg_perf_eps_greedy, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEps-Greedy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_perf_dyna_qplus' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot Figure 6 from Sutton (1990) here\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(avg_perf_dyna_qplus,  label='Dyna-Q+')\n",
    "plt.plot(avg_perf_dyna_qminus, label='Dyna-Q-')\n",
    "plt.plot(avg_perf_eps_greedy, label='Eps-Greedy')\n",
    "plt.axvline(3000, linestyle='--', c='r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe in your own words the apparent differences you observe in the above plot. Some of those differences involve the particular choice of the exploration bonus used in this algorithm. Suggest and implement another sensible exploration bonus and compare the performance of your agent against Dyna-$Q$+ and Dyna-$Q$-. Explain your choice. \n",
    "\n",
    "Dyna Q-plus will perform better since a bigger epsilon will increase the exploration. This is especially efficient when the world is changing. Dyna Q- doesn't have any exploration, this means if the world changes the agent won't find the shortcut since it always takes the path which the agent thought was best before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-step task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2-step task is one of the most iconic RL tasks (Daw et al. 2011). It was designed to dissect the relative contributions of the MF and MB systems in human choices. There are multiple ways in which MB information can enter choice. For instance, as you will have seen in the case of Dyna, the MF values are additionally trained by the MB system during offline behavioural states. In fact, this process of Dyna-style planning parallels closely hippocampal replay which has been suggested to implement MB planning (Mattar \\& Daw, 2018).\n",
    "\n",
    "For the purpose of this exercise, we will assume that the choice is guided by a linear combination of the MF and MB values. Thus, by tweaking the relative contribution of each, you would expect different behaviours to emerge. Classically, the measure of this balance used in the 2-step task is stay probability. That is, the probability that the subject/agent repeats the same first-stage choice conditioned on the outcome of the second stage in the previous trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2 [20 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part of the assigniment, **your task is to reproduce and visualise data plotted in figure 2** in Daw et al. (2011). There is no pre-implemented code for the agent, except for some basics in the `agent.py` file where you can find the `TwoStepAgent` class. Therefore, you have to follow the methodology in the paper and implement it yourself. You can find the relevant paper in the `papers` folder of this git repository. The only provided code is the one below, as well as the `get_stay_probabilities` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import TwoStepAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the agent. The parameters are taken from the paper\n",
    "agent = TwoStepAgent(alpha1=0.54, alpha2=0.42, beta1=5.19, beta2=3.69, lam=0.57, w=0.39, p=0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-step task\n",
    "def create_dict():\n",
    "    \"\"\" Define the different states as dictionaries. \n",
    "                - name is the name of the state\n",
    "                - state_num is the number of the state\n",
    "                - Boolean if there is a reward\n",
    "                - the possbile actions in the state\n",
    "                - the name of the actions\n",
    "                - the beta parameter for the softmax function \"\"\"\n",
    "                    \n",
    "    \"\"\" Q-values for the model free, model based and combination of both.\n",
    "    The Q-values are stored in a 2D array. The first dimension is the state,\n",
    "    the second the action.\"\"\"\n",
    "    Q_MF =np.zeros((3,2))   \n",
    "    Q_MB =np.zeros((3,2))\n",
    "    Q_net =np.zeros((3,2))         \n",
    "                \n",
    "    stage_c = { 'name': 'stage_c',        'state_num' : 2,                      'reward' : True,                \n",
    "                'action' : [0, 1],        'action_name' : ['c0', 'c1'],        \n",
    "                'beta' : 0.2}\n",
    "    \n",
    "    stage_b = { 'name': 'stage_b',        'state_num' : 1,                      'reward' : True,               \n",
    "                'action' : [0, 1],        'action_name' : ['b0', 'b1'],         \n",
    "                'beta' : 0.2}\n",
    "    \n",
    "    stage_a = { 'name': 'stage_a',        'state_num' : 0,                      'reward' : False, \n",
    "                'action' : [0, 1],        'action_name' : ['a0', 'a1'],         \n",
    "                'beta' : 0.2}\n",
    "\n",
    "\n",
    "    \"\"\" Define the connections between the states and action. In stage_a we have 2\n",
    "        actions which lead to the different stages b and c. Further, the transition     \n",
    "        probabilities of entering these stages are defined.\"\"\"\n",
    "    Connection_dict = { 'stage_a':  {'action' :     {0 : [stage_b , stage_c], \n",
    "                                                     1: [stage_b , stage_c]},\n",
    "                                    'trans_prob' :  {0 : [[0.7, 0.3],[0.3, 0.7]],\n",
    "                                                     1 : [[0.3, 0.7],[0.7, 0.3]]}},\n",
    "                                    \n",
    "                        'stage_b' : {'action' :     {0 : [], 1 : []},\n",
    "                                     'trans_prob' :  0},\n",
    "                        'stage_c' : {'action' :     {0 : [], 1 : []},\n",
    "                                     'trans_prob' :  0}}\n",
    "        \n",
    "    return stage_a, stage_b, stage_c, Connection_dict, Q_MF, Q_MB, Q_net\n",
    "\n",
    "stage_a, stage_b, stage_c, Connection_dict, Q_MF, Q_MB, Q_net = create_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Caluculate the reward. \"\"\"\n",
    "def reward(current_state, action):\n",
    "    \"\"\" If there is an reward in the current state. \"\"\"\n",
    "    if current_state['reward']:\n",
    "        \n",
    "        \"\"\" Add gausian Noise to the reward \"\"\"\n",
    "        gaussian_noise = np.random.normal(0, 0.025)\n",
    "        \"\"\" Make the reward commulative. \"\"\"\n",
    "        current_state['reward_prob'][action] +=  gaussian_noise\n",
    "        \n",
    "        \"\"\" Create a reflective border at 0.25 and 0.75 for th reward probability \"\"\"\n",
    "        if current_state['reward_prob'][action] > 0.75:\n",
    "            current_state['reward_prob'][action] = 0.75 - (current_state['reward_prob'][action] - 0.75)\n",
    "        elif current_state['reward_prob'][action] < 0.25:\n",
    "            current_state['reward_prob'][action] = 0.25 + (0.25 - current_state['reward_prob'][action])\n",
    "            \n",
    "        \n",
    "        \"\"\" Get the reward \"\"\"               \n",
    "        reward = np.random.choice([1, 0], p = [current_state['reward_prob'][action], 1 - current_state['reward_prob'][action]])\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "        \"\"\" If there is no reward in state return 0. \"\"\"\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Update the Q-Model free updates. The update is done according to the Sutton paper.\"\"\"\n",
    "def Q_MF_update(Q_MF, current_state, action, next_state, next_action, alpha, lambda_val):\n",
    "    \n",
    "    \"\"\" If we are not in the first stage\"\"\"\n",
    "    if current_state['state_num'] != 0:\n",
    "        delta = reward(next_state, next_action) - Q_MF[current_state['state_num'], action]\n",
    "        Q_MF[current_state['state_num'], action] +=  alpha * delta\n",
    "                \n",
    "        \"\"\" If we are in the first state\"\"\"\n",
    "    elif current_state['state_num'] == 0:\n",
    "        delta =  Q_MF[next_state['state_num'], next_action] - Q_MF[current_state['state_num'], action]\n",
    "        Q_MF[current_state['state_num'], action] +=  alpha * lambda_val * delta\n",
    "        \n",
    "        \"\"\" If we are in an empty state. \"\"\"\n",
    "    else:\n",
    "        Q_MF[current_state['state_num'], action] = 0\n",
    "    \n",
    "    return Q_MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Update the Q-Model free updates. The update is done according to the Sutton paper.\"\"\"\n",
    "def Q_MB_update(Q_MB, current_state, action, prob_order):\n",
    "    \n",
    "    \"\"\" If we are in the first state, include the transition probabilities\"\"\"\n",
    "    if current_state['state_num'] == 0:\n",
    "        \n",
    "        P_stage_b = Connection_dict[stage_a['name']]['trans_prob'][action][prob_order]\n",
    "        P_stage_c = Connection_dict[stage_a['name']]['trans_prob'][action][prob_order]\n",
    "        \n",
    "        \"\"\" Get the maximum Q-value of the next state and actions. \"\"\"\n",
    "        Q_MB[current_state['state_num'], action] = P_stage_b  * np.max(Q_MF[1,0], Q_MF[1,1]) + P_stage_c * np.max(Q_MF[2,0], Q_MF[2,1])\n",
    "     \n",
    "        \"\"\" If we are not in the second state the Q_MB equals the Q_MF\"\"\"   \n",
    "    elif current_state['state_num'] == 1:\n",
    "        \n",
    "        Q_MB[current_state['state_num'], action] = Q_MF[current_state['state_num'], action]\n",
    "\n",
    "    \n",
    "    return Q_MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Update the Q_net value. \"\"\"\n",
    "def Q_net(Q_net, stage, action, Q_MF, Q_MB, w):\n",
    "    Q_net[stage['state_num'], action] = w * Q_MF[stage['state_num'], action] + (1-w) * Q_MB[stage['state_num'], action]\n",
    "    return Q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Check if the last action is the same as the current action and if we\n",
    "    we are in the first stage.\"\"\"\n",
    "def rep(stage, possible_action, last_action):\n",
    "    \"\"\" Check if any last action was executed. \"\"\"\n",
    "    if last_action == -1:\n",
    "        return 0\n",
    "    elif (stage['name'] == 'stage_a') and (possible_action == last_action):\n",
    "       return 1\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Select the actions. \"\"\"\n",
    "def select_action(stage, last_action, Q_net, prob_order, p):\n",
    "    \n",
    "    p_a_given_s = []\n",
    "    Nominator, Denominator = 0, 0\n",
    "    \n",
    "    \"\"\" Use a softmax function to calulate the probabiltiies of the actions by including the Q_net values, p and the rep function. \"\"\"\n",
    "    for possible_action in stage['action']:\n",
    "        Nominator = np.exp(stage['beta'] * Q_net[stage['state_num'], possible_action] + p * rep(stage,possible_action, last_action)) \n",
    "        \n",
    "        Denominator =  sum(np.exp(stage['beta'] * Q_net[stage['state_num'], action] + p * rep(stage,action, last_action)) for action in stage['action']) \n",
    "        prob = Nominator/Denominator\n",
    "        \"\"\" Append the probability to the list\"\"\"\n",
    "        p_a_given_s.append(prob)\n",
    "        \n",
    "   \n",
    "    \"\"\" Weigh with this the possible action \"\"\"\n",
    "    action = np.random.choice(stage['action'], p = p_a_given_s)\n",
    "    \n",
    "    \"\"\" Get the next stage from the connection_dict, by taking into account the transition probabilities.\"\"\"\n",
    "    if Connection_dict[stage['name']]['action'][action] != []:\n",
    "        next_state = np.random.choice(Connection_dict[stage['name']]['action'][action], p = Connection_dict[stage['name']]['trans_prob'][prob_order][action])\n",
    "    else:\n",
    "        next_state = []\n",
    "    \n",
    "    \n",
    "    return action, next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Look how aften action a and b was choosen and switch transitions probabilities,\n",
    "    if one action was chosen more often. \"\"\"\n",
    "def count_decide_prob(action, next_state):\n",
    "    \n",
    "    if (action == 0 and next_state['name'] == 'stage_b') or (action == 1 and next_state['name'] == 'stage_c'):\n",
    "        counter_1 += 1\n",
    "    elif (action == 0 and next_state['name'] == 'stage_C') or (action == 1 and next_state['name'] == 'stage_b'):\n",
    "        counter_2 += 1\n",
    "        \n",
    "    if counter_1 > counter_2:\n",
    "        prob_order = 0\n",
    "    else:\n",
    "        prob_order = 1\n",
    "        \n",
    "    return prob_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\flori\\Dokumente\\Studium Tuebingen\\Inhalt\\Master\\1. Semester\\Neural Modelling\\Woche 9\\assignment.ipynb Cell 30\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m prob_order \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m counter_1, counter_2 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m action, next_state \u001b[39m=\u001b[39m select_action(current_state, last_action, Q_net, prob_order, p \u001b[39m=\u001b[39;49m \u001b[39m0.5\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m prob_order \u001b[39m=\u001b[39m count_decide_prob(action, next_state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m last_action \u001b[39m=\u001b[39m action\n",
      "\u001b[1;32mc:\\Users\\flori\\Dokumente\\Studium Tuebingen\\Inhalt\\Master\\1. Semester\\Neural Modelling\\Woche 9\\assignment.ipynb Cell 30\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m Nominator, Denominator \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m possible_action \u001b[39min\u001b[39;00m stage[\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     Nominator \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(stage[\u001b[39m'\u001b[39m\u001b[39mbeta\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m Q_net[stage[\u001b[39m'\u001b[39;49m\u001b[39mstate_num\u001b[39;49m\u001b[39m'\u001b[39;49m], possible_action] \u001b[39m+\u001b[39m p \u001b[39m*\u001b[39m rep(stage,possible_action, last_action)) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     Denominator \u001b[39m=\u001b[39m  \u001b[39msum\u001b[39m(np\u001b[39m.\u001b[39mexp(stage[\u001b[39m'\u001b[39m\u001b[39mbeta\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m Q_net[stage[\u001b[39m'\u001b[39m\u001b[39mstate_num\u001b[39m\u001b[39m'\u001b[39m], action] \u001b[39m+\u001b[39m p \u001b[39m*\u001b[39m rep(stage,action, last_action)) \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m stage[\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     prob \u001b[39m=\u001b[39m Nominator\u001b[39m/\u001b[39mDenominator\n",
      "\u001b[1;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\"\"\" For testing \"\"\"\n",
    "agent._init_history()\n",
    "\n",
    "current_state = stage_a\n",
    "\n",
    "last_action = -1\n",
    "prob_order = -1\n",
    "counter_1, counter_2 = 0, 0\n",
    "\n",
    "action, next_state = select_action(current_state, last_action, Q_net, prob_order, p = 0.5)\n",
    "            \n",
    "prob_order = count_decide_prob(action, next_state)\n",
    "\n",
    "last_action = action\n",
    "\n",
    "next_action, _ = select_action(next_state, last_action, Q_net, prob_order, p = 0.5)\n",
    "\n",
    "r = reward(next_state, next_action)\n",
    "\n",
    "Q_MF = Q_MF_update(Q_MF, current_state, action, next_state, next_action, alpha = 0.5, lambda_val = 0.5)\n",
    "Q_MF = Q_MF_update(Q_MF, next_state, next_action, _, _, alpha = 0.5, lambda_val = 0.5)\n",
    "\n",
    "Q_MB = Q_MB_update(Q_MB, current_state, action, prob_order)\n",
    "Q_MB = Q_MB_update(Q_MB, next_state, next_action, prob_order)\n",
    "\n",
    "Q_net = Q_net(Q_net, current_state, action, Q_MF, Q_MB, w = 0.5)\n",
    "Q_net = Q_net(Q_net, next_state, next_action, Q_MF, Q_MB, w = 0.5)\n",
    "\n",
    "agent._update_history(next_action, next_state, r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TwoStepAgent' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\flori\\Dokumente\\Studium Tuebingen\\Inhalt\\Master\\1. Semester\\Neural Modelling\\Woche 9\\assignment.ipynb Cell 31\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X41sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_averg):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     agent\u001b[39m.\u001b[39msimulate(num_trials)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     stay_probas[n, :] \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_stay_probabilities()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     current_state \u001b[39m=\u001b[39m stage_a\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/flori/Dokumente/Studium%20Tuebingen/Inhalt/Master/1.%20Semester/Neural%20Modelling/Woche%209/assignment.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mfor\u001b[39;00m trial \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_trials):\n",
      "File \u001b[1;32mc:\\Users\\flori\\Dokumente\\Studium Tuebingen\\Inhalt\\Master\\1. Semester\\Neural Modelling\\Woche 9\\agent.py:301\u001b[0m, in \u001b[0;36mTwoStepAgent.get_stay_probabilities\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m rare_nr       \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    299\u001b[0m num_rare_nr   \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 301\u001b[0m num_trials \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    302\u001b[0m \u001b[39mfor\u001b[39;00m idx_trial \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_trials\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    303\u001b[0m     a, s1, r1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory[idx_trial, :]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TwoStepAgent' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "# run simulations\n",
    "num_trials  = 201\n",
    "num_averg   = 17\n",
    "stay_probas = np.zeros((num_averg, 4))\n",
    "for n in range(num_averg):\n",
    "    agent.simulate(num_trials)\n",
    "    stay_probas[n, :] = agent.get_stay_probabilities()\n",
    "    \n",
    "    \"\"\" Initialize the agent. \"\"\"\n",
    "    agent._init_history()\n",
    "\n",
    "    \"\"\" Intialize the first state\"\"\"\n",
    "    current_state = stage_a\n",
    "\n",
    "    \"\"\" Initialize the last action, the prob_order and counter\"\"\"\n",
    "    last_action = -1\n",
    "    prob_order = -1\n",
    "    counter_1, counter_2 = 0, 0\n",
    "\n",
    "    \"\"\" Chose action and next state\"\"\"\n",
    "    action, next_state = select_action(current_state, last_action, Q_net, prob_order, p = 0.5)\n",
    "                \n",
    "    \"\"\" Update the transition probabilities order \"\"\"\n",
    "    prob_order = count_decide_prob(action, next_state)\n",
    "\n",
    "    \"\"\" Set last action in first stage as action \"\"\"\n",
    "    last_action = action\n",
    "\n",
    "    \"\"\" Get the next action and next state. Next state isn't used.\"\"\"\n",
    "    next_action, _ = select_action(next_state, last_action, Q_net, prob_order, p = 0.5)\n",
    "\n",
    "    \"\"\" Get reward for second state and second action \"\"\"\n",
    "    r = reward(next_state, next_action)\n",
    "\n",
    "    \"\"\" Updatae Q-Model free values. \"\"\"\n",
    "    Q_MF = Q_MF_update(Q_MF, current_state, action, next_state, next_action, alpha = 0.5, lambda_val = 0.5)\n",
    "    Q_MF = Q_MF_update(Q_MF, next_state, next_action, _, _, alpha = 0.5, lambda_val = 0.5)\n",
    "\n",
    "    \"\"\" Updatae Q-Model based values. \"\"\"\n",
    "    Q_MB = Q_MB_update(Q_MB, current_state, action, prob_order)\n",
    "    Q_MB = Q_MB_update(Q_MB, next_state, next_action, prob_order)\n",
    "\n",
    "    \"\"\" Updatae Q-Model net values. \"\"\"\n",
    "    Q_net = Q_net(Q_net, current_state, action, Q_MF, Q_MB, w = 0.5)\n",
    "    Q_net = Q_net(Q_net, next_state, next_action, Q_MF, Q_MB, w = 0.5)\n",
    "\n",
    "    \"\"\" Update the history of the agent. \"\"\"\n",
    "    agent._update_history(next_action, next_state, r)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(1, np.mean(stay_probas[:, 0]), facecolor='blue', label='common')\n",
    "plt.bar(2, np.mean(stay_probas[:, 1]), facecolor='red', label='rare')\n",
    "plt.bar(3, np.mean(stay_probas[:, 2]), facecolor='blue')\n",
    "plt.bar(4, np.mean(stay_probas[:, 3]), facecolor='red')\n",
    "plt.ylim(0.5, 1)\n",
    "plt.xticks([1.5, 3.5], ['rewarded', 'unrewarded'])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe in your own words the apparent differences between the MF and MB agents. What do the data plotted with best-fitting parameters tell you about the relative contributions of MF and MB systems to subjects' choices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphic shows that the model free or reinforcement algorithm doesn't take the causal strucutre of the probabilities into account since if there was a wrong prediction this prediction is backwards propagted to the step before. The model-based algorithm creates a causal structure of the envrionemnt. Therefore, the agent takes the probabilities into account and the stay probabilities differ dependent on, if the transition probability is common or rare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
